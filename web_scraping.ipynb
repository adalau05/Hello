{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQUMoQvILFZlRki67cXqMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalau05/Hello/blob/main/web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-KsspLsq_Co"
      },
      "source": [
        "#perform google search, then put all url in a list\n",
        "from googlesearch import search\n",
        "query = '\"Lu Chi Su\"'#put last name at back\n",
        "url_list=[] #define a list called url_list\n",
        "for i in search(query, lang=\"en\", tld=\"com\", num=10, stop=20, pause=2):\n",
        "  if (i[-3:] != \"pdf\"): #ignore pdf websites\n",
        "    #print(i)\n",
        "    url_list.append(i) #append url to url_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSGn8EdnuO6d"
      },
      "source": [
        "#save the url list to excel\n",
        "import pandas as pd\n",
        "url_df = pd.DataFrame([url_list])\n",
        "url_df1 = url_df.transpose()\n",
        "url_df1.columns = ['url']\n",
        "url_df1.to_excel(\"url.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6u2_gfNA6e1",
        "outputId": "baa259a7-f0bf-44c5-abcf-ef6402bd25af"
      },
      "source": [
        "pip install trafilatura"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: justext>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (2.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from trafilatura) (2021.5.30)\n",
            "Requirement already satisfied: readability-lxml>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (0.8.1)\n",
            "Requirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (4.6.3)\n",
            "Requirement already satisfied: courlan>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (0.4.2)\n",
            "Requirement already satisfied: urllib3<2,>=1.25 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (1.26.6)\n",
            "Requirement already satisfied: chardet>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (4.0.0)\n",
            "Requirement already satisfied: htmldate>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (0.9.0)\n",
            "Requirement already satisfied: tld in /usr/local/lib/python3.7/dist-packages (from courlan>=0.4.2->trafilatura) (0.12.6)\n",
            "Requirement already satisfied: regex>=2021.4.4 in /usr/local/lib/python3.7/dist-packages (from htmldate>=0.9.0->trafilatura) (2021.8.28)\n",
            "Requirement already satisfied: dateparser>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from htmldate>=0.9.0->trafilatura) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from htmldate>=0.9.0->trafilatura) (2.8.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0->htmldate>=0.9.0->trafilatura) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0->htmldate>=0.9.0->trafilatura) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.1->htmldate>=0.9.0->trafilatura) (1.15.0)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (from readability-lxml>=0.8.1->trafilatura) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiFJbnHyE-Um"
      },
      "source": [
        "#read url.xlsx file\n",
        "import pandas as pd\n",
        "df = pd.read_excel('url.xlsx')\n",
        "df.columns = [\"num\", \"url\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq-D8W_DAxc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b60b09a-bd6a-49bc-c079-8d733c778571"
      },
      "source": [
        "#iterate through urls, extract contents and save to txt file, took 7 mins for all 60 urls\n",
        "import trafilatura\n",
        "\n",
        "j=0 #integer for incrementing filenames\n",
        "\n",
        "for i in range(len(df[\"url\"])) : #iterate url dataframe\n",
        "  try:\n",
        "    downloaded = trafilatura.fetch_url(df[\"url\"][i]) #open url\n",
        "    text = trafilatura.extract(downloaded)  #extract content from url if possible\n",
        "    #print(text)\n",
        "    with open(\"content_%s.txt\" % j, 'w') as f: #create content file\n",
        "      f.write(text) #save content to txt file\n",
        "  except:\n",
        "    print(\"Sorry! Cannot extract content from this webpage.\") #print on screen if cannot extract content of url\n",
        "  j = j+1 #iterate for next filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry! Cannot extract content from this webpage.\n",
            "Sorry! Cannot extract content from this webpage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJLdWoAT-FDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431d4396-6f70-4a65-db16-ff3e93eb064c"
      },
      "source": [
        "#iterate through content txt files, perform ner, find position of occurrence and word count frequency, and save to excel\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "k=0 #iterate content files\n",
        "frequency_list = []\n",
        "for k in range(len(df[\"url\"])) :\n",
        "  try:\n",
        "    with open('content_%s.txt' % k) as f:\n",
        "      text = f.read()\n",
        "      doc = nlp(text)\n",
        "      for ent in doc.ents:\n",
        "        s = [ent.text, ent.label_, ent.start_char, text.count(ent.text), k]\n",
        "        frequency_list.append(s)\n",
        "      #print(frequency_list)\n",
        "      frequency_df = pd.DataFrame(frequency_list,columns=['Text', \"Part_of_speech\", \"position\", \"frequency\", \"url_index\"])\n",
        "      frequency_df.to_excel(\"frequency.xlsx\")\n",
        "  except:\n",
        "    print(\"Sorry! File not found.\")\n",
        "  k=k+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u24RfPbv7fyq"
      },
      "source": [
        "#define ner and tokenization library and functions\n",
        "\n",
        "class EntityRetokenizeComponent:\n",
        "  def __init__(self, nlp):\n",
        "    pass\n",
        "  def __call__(self, doc):\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for ent in doc.ents:\n",
        "            retokenizer.merge(doc[ent.start:ent.end])\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "retokenizer = EntityRetokenizeComponent(nlp)\n",
        "nlp.add_pipe(retokenizer, name='merge_phrases', last=True)\n",
        "\n",
        "#open content txt files, tokenize name entities, remove stop words, find part of speech and save results to excel\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "k=0 #iterate content files\n",
        "ner_list=[] #create ner list\n",
        "\n",
        "for k in range(len(df[\"url\"])) :\n",
        "  try:\n",
        "    with open('content_%s.txt' % k) as f:\n",
        "      contents = f.read()\n",
        "      doc = nlp(contents)\n",
        "\n",
        "      tokenized_text = [tok for tok in doc if not tok.is_stop and not tok.is_punct]\n",
        "      #print(tokenized_text)\n",
        "\n",
        "      for token in tokenized_text:\n",
        "        t=[token.text, token.pos_, token.idx, k]\n",
        "        #print(t)\n",
        "        ner_list.append(t)\n",
        "  except:\n",
        "    print(\"Sorry! File not found\")\n",
        "  k = k+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03JDAqJdMIte"
      },
      "source": [
        "#save result to excel\n",
        "ner_df = pd.DataFrame(ner_list)\n",
        "ner_df.columns = ['Text', 'Part_of_Speech', 'Position', \"url_index\"]\n",
        "ner_df.to_excel('ner_pos.xls')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB04bvJclUm2",
        "outputId": "6c2d8a73-65f0-4b93-9b3e-db78a31c6ed9"
      },
      "source": [
        "!pip install pandasql"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandasql\n",
            "  Downloading pandasql-0.7.3.tar.gz (26 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.1.5)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from pandasql) (1.4.22)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandasql) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.15.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->pandasql) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->pandasql) (3.7.4.3)\n",
            "Building wheels for collected packages: pandasql\n",
            "  Building wheel for pandasql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandasql: filename=pandasql-0.7.3-py3-none-any.whl size=26781 sha256=ee96cf13fb083a8904db028f060619ce5592009209a85fdab43623377fe24768\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/4b/ec/41f4e116c8053c3654e2c2a47c62b4fca34cc67ef7b55deb7f\n",
            "Successfully built pandasql\n",
            "Installing collected packages: pandasql\n",
            "Successfully installed pandasql-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hXzFe-rQtsD"
      },
      "source": [
        "import pandasql as ps\n",
        "\n",
        "#to read table\n",
        "df=pd.read_excel(\"ner_pos.xls\")\n",
        "\n",
        "df['prev_Text'] = df['Text'].shift(-1)\n",
        "df['next_Text'] = df['Text'].shift(1)\n",
        "\n",
        "#run sql on dataframe\n",
        "output_df  = ps.sqldf(\"SELECT prev_text,Text,next_Text,url_index from df WHERE Text = 'wife'\")\n",
        "\n",
        "df2=pd.read_excel(\"url.xlsx\")\n",
        "\n",
        "join_url = output_df.join(df2, on=\"url_index\")\n",
        "join_url\n",
        "\n",
        "result = ps.sqldf(\"SELECT prev_text,Text,next_Text,url from join_url\")\n",
        "result\n",
        "\n",
        "writer = pd.ExcelWriter('result.xlsx')\n",
        "result.to_excel(writer)\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5IHVmcpvx3l"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}